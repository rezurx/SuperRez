"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.LocalAIManager = void 0;
const vscode = require("vscode");
const child_process_1 = require("child_process");
const util_1 = require("util");
const execAsync = (0, util_1.promisify)(child_process_1.exec);
class LocalAIManager {
    constructor() {
        this.providers = new Map();
        this.outputChannel = vscode.window.createOutputChannel('SuperRez Local AI');
        this.initializeProviders();
    }
    initializeProviders() {
        // Ollama (if available)
        this.providers.set('ollama', {
            name: 'Ollama',
            command: 'ollama',
            available: false,
            models: ['llama3.1:8b', 'deepseek-coder:6.7b', 'codellama:7b'],
            costPerToken: 0.0
        });
        // Local Python transformers (lightweight)
        this.providers.set('local-transformers', {
            name: 'Local Transformers',
            command: 'python3',
            available: false,
            models: ['microsoft/DialoGPT-medium', 'Salesforce/codegen-350M-mono'],
            costPerToken: 0.0
        });
        // Mock local provider for testing
        this.providers.set('mock-local', {
            name: 'Mock Local AI',
            command: 'echo',
            available: true,
            models: ['mock-coder', 'mock-analyst'],
            costPerToken: 0.0
        });
    }
    async detectAvailableProviders() {
        const available = [];
        for (const [key, provider] of this.providers) {
            try {
                if (key === 'ollama') {
                    // Check if Ollama is installed
                    await execAsync('which ollama');
                    provider.available = true;
                }
                else if (key === 'local-transformers') {
                    // Check if transformers is available
                    await execAsync('python3 -c "import transformers"');
                    provider.available = true;
                }
                else if (key === 'mock-local') {
                    // Always available for testing
                    provider.available = true;
                }
                if (provider.available) {
                    available.push(provider);
                }
            }
            catch (error) {
                provider.available = false;
                this.outputChannel.appendLine(`${provider.name} not available: ${error}`);
            }
        }
        return available;
    }
    async generateCode(provider, prompt, model) {
        const providerObj = this.providers.get(provider);
        if (!providerObj || !providerObj.available) {
            throw new Error(`Provider ${provider} not available`);
        }
        try {
            switch (provider) {
                case 'ollama':
                    return await this.generateWithOllama(prompt, model || 'llama3.1:8b');
                case 'local-transformers':
                    return await this.generateWithTransformers(prompt, model || 'microsoft/DialoGPT-medium');
                case 'mock-local':
                    return await this.generateWithMock(prompt);
                default:
                    throw new Error(`Unknown provider: ${provider}`);
            }
        }
        catch (error) {
            this.outputChannel.appendLine(`Error generating with ${provider}: ${error}`);
            throw error;
        }
    }
    async generateWithOllama(prompt, model) {
        const command = `ollama run ${model} "${prompt.replace(/"/g, '\\"')}"`;
        const { stdout } = await execAsync(command, { timeout: 30000 });
        return stdout.trim();
    }
    async generateWithTransformers(prompt, model) {
        // Create a simple Python script for local generation
        const pythonScript = `
import sys
from transformers import AutoTokenizer, AutoModel
import torch

try:
    # Simple generation using transformers
    print("Local AI: Generated response for: ${prompt.substring(0, 50)}...")
    print("This would use ${model} for actual generation.")
    print("For demo: Here's a code suggestion based on your prompt.")
except Exception as e:
    print(f"Error: {e}")
`;
        const { stdout } = await execAsync(`python3 -c "${pythonScript}"`, { timeout: 15000 });
        return stdout.trim();
    }
    async generateWithMock(prompt) {
        // Mock local AI response for testing
        const responses = [
            `// Generated by Mock Local AI
// Based on prompt: ${prompt.substring(0, 50)}...

function exampleFunction() {
    // This is a mock response demonstrating local AI capability
    // In real implementation, this would be generated by a local model
    return "Hello from local AI!";
}`,
            `# Mock Local AI Response
# Analyzing: ${prompt.substring(0, 50)}...

def analyze_code():
    """
    Mock analysis from local AI provider.
    This demonstrates zero-cost local processing.
    """
    return {
        'status': 'analyzed',
        'suggestions': ['Use local AI for cost savings', 'Great pattern detected'],
        'cost': 0.00
    }`,
            `// Local AI Code Review
// Prompt: ${prompt.substring(0, 50)}...

const codeReview = {
    security: "No issues found",
    performance: "Optimal patterns detected", 
    maintainability: "Good code structure",
    cost: 0.00,
    provider: "Local AI"
};`
        ];
        // Simulate processing time
        await new Promise(resolve => setTimeout(resolve, 500));
        const randomResponse = responses[Math.floor(Math.random() * responses.length)];
        return randomResponse;
    }
    async getProviderSummary() {
        const providers = await this.detectAvailableProviders();
        let summary = `# Local AI Providers Summary\n\n`;
        summary += `**Available Providers**: ${providers.length}\n\n`;
        for (const provider of providers) {
            summary += `## ${provider.name}\n`;
            summary += `- **Status**: ${provider.available ? '‚úÖ Available' : '‚ùå Not Available'}\n`;
            summary += `- **Cost**: $${provider.costPerToken.toFixed(4)} per token\n`;
            summary += `- **Models**: ${provider.models.join(', ')}\n`;
            summary += `- **Command**: \`${provider.command}\`\n\n`;
        }
        if (providers.length === 0) {
            summary += `## Installation Options\n\n`;
            summary += `To enable local AI capabilities:\n\n`;
            summary += `1. **Install Ollama**: \`curl -fsSL https://ollama.com/install.sh | sh\`\n`;
            summary += `2. **Install Python transformers**: \`pip install transformers torch\`\n`;
            summary += `3. **Use Mock Provider**: Available for testing (current)\n\n`;
        }
        summary += `**Benefits of Local AI**:\n`;
        summary += `- üÜì Zero API costs\n`;
        summary += `- üîí Complete privacy\n`;
        summary += `- ‚ö° No internet required\n`;
        summary += `- üéØ Optimized for coding tasks\n\n`;
        summary += `Generated by SuperRez Local AI Manager`;
        return summary;
    }
}
exports.LocalAIManager = LocalAIManager;
//# sourceMappingURL=localAI.js.map